## 2021-12 李宏毅2021

1. sigmoid翻译过来大概是S型曲线的意思，函数图像就是一个s型曲线，sigmoid(x) =1/(1+e(-x))，用多个比较有弹性的sigmoid就可以来逼近任何一条曲线，为了让sigmoid更有弹性，加几个参数，所以最后的样子应该是：c/(1+e(b+wx))，c控制高度，b控制左右，w控制S型曲线的弯曲程度。逼近的原理是：把任何曲线用piecewise linear curves来表示（类似微积分），然后用多个hard sigmoid就可以精确组合成任何piecewise linear curves，但是hard sigmoid是分段函数，很难写出表达式，所以用sigmoid去逼近hard sigmoid。多个sigmoid求和，再加上一个b， $y=b+\sum_{i}{c_i/(1+e^{-(b_i+w_i{x})}})=b+\sum_{i}{c_i}sigmoid(b_i+w_ix)$
2. 每个sigmoid(x)，里面的x就写作 $b+wx$，考虑到多个维度： $y=b+\sum_{i}{c_i}sigmoid(b_i+\sum_{j}w_{ij}x)$，很多参数全部写成一个列向量里，就是 $\theta$，梯度就是 $g={\nabla}L(\theta^0)$
3. 随机梯度下降，就是把训练集变成多个batch。
4. learning rate、几个sigmoid、batch数等都是hyper parameter
5. hard sigmoid可以用**两倍数量**的Relu来模拟

### 李宏毅：pytorch教学

4. pytorch常见问题： 
   1. tensor在不同device
   2. mismatched dimensions（需要用transpose, squeeze, unsqueeze）
   3. cuda out of  memory（如果data是iterated的，或者使用Dataloader都可以解决问题==（？）==）
   4. mismatched tensor type（使用.long()等转换类型）

### 李宏毅：HW1

1. 用美国的covid-19数据：收集到的资料和检测出阳性的比例
2. evaluation metric：RMSE
3. kaggle的使用：区分public和private leaderboard
4. 如果代码有引用，需要在代码最后注明（如Source: Zi-Qi Li @ NTUEE (https://xxx.com)）
5. 将ipynb文件用colab的上传功能上传，会被自动加载到google drive的colab notebook。
6. 建议文件都放在google drive上
7. 学习csv读写：list(csv.reader())
8. 正则化的时候，减去mean除以std，记得保证keepdim=True
9. 可以用self.criterion = nn.MSELoss()来设置损失函数，也可以单独定义函数cal_loss，然后计算criterion==（？）==
10. 学习nn.MSELoss()： 
   1. 一般用于 $m\times1$ 的input和 $m\times1$ 的target，然后用reduction='mean'，就会得到一个压缩后的结果。
   2. 如果target的第二维长度不为1，那就必须和input的第二维长度相同
11. 梯度下降：data--(net)-->pred，pred--(与target对比、压缩)-->loss，loss--(对data里的各个参数求导(backward))-->grad
12. 机器学习的一个思想，就是假定函数，然后用现实中的数据作为”参数“，把假定的函数的参数作为”自变量“，去对参数求导、找最好的参数，有一种颠倒的感觉
13. backward就是backpropagation（？）
14. 对criterion调用backward之后，直接用optimizer.step()即可，不需要打印data的导数，封装得死死的
15. 连batch_size都被封装dataloader里面了，在训练的时候直接for x, y in tr_set就行（应该是这里每次循环一个batch进行update），不用手动分batch，封装得死死的
16. validation计算loss的时候，也是for x, y in dv_set，这里每次循环是一个batch，但是最后可能会有的batch比较小，所以每次循环里面要乘以batch长度，最后再除以资料总数量
17. 记录loss_record的时候常常用到detach()，cpu(), item()
18. 通过models.pth记录训练中比较好的模型（每次epoch都用验证集验证，如果loss小于此前最小的loss就记录，如果超过early_stop_cnt次epoch都没有缩小的话，就直接early_stop就好）
19. 选择损失函数的时候，除了直接用MSELoss，还可以加入[L1 and L2 Regularization Methods. Machine Learning | by Anuja Nagpal | Towards Data Science](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)
20. torch.norm的正则，对weight和bias等所有参数都进行了惩罚，但是定义来说L1和L2正则应该只对weight惩罚
21. 学习自定义正则 [(49条消息) pytorch实现L2和L1正则化regularization的方法_pan_jinquan的博客-CSDN博客](https://blog.csdn.net/guyuealian/article/details/88426648)
22. 用getattr可以帮助pytorch.optim选择使用的优化器，然后用**config，可以选择配置。
23. 遍历model.named_parameters()，可以得到name和param
24. 用torch.linalg.norm(weight, ord=2)来正则化

### colab使用注意：

1. 每12小时需要重新挂载，意味着模型训练会中断，因此需要**设置checkpoint**（？）
2. 使用一小段时间会断开连接，短时间内重新连接则模型训练不会中断，建议使用**按键精灵**按f10启动脚本左键点击，或者谷歌应用市场里的拓展工具Auto reconnect colab==（？）==
3. "代码会在专供您的帐号使用的虚拟机中执行。虚拟机闲置一段时间后会被删除，并且 Colab 服务为虚拟机强制设置了最长有效期。"[Google Colab](https://research.google.com/colaboratory/faq.html#idle-timeouts)
4. 避免太多琐碎的 I/O 读取，建议选择以压缩格式（例如：`.zip` 或 `.tar.gz` 文件）将数据从云端硬盘复制到 Colab 虚拟机，然后在虚拟机本地而非装载的云端硬盘目录中解压缩数据。[Google Colab](https://research.google.com/colaboratory/faq.html#idle-timeouts)

### 李宏毅：deep learning简介

1. 2006：RBM machine，受限玻尔兹曼机（已经没啥用了）
2. 2009：GPU
3. deep learning：sigmoid(wb+x)套娃，写成矩阵运算，就可以用GPU做加速
4. hidde layers就相当于一个feature extrator，可以取代以前的feature engineering
5. output layer一般用softmax，相当于一个classifier
6. deep learning有两种思路： 
   1. 寻找好的feature
   2. 寻找好的network structure
7. 深度学习就只有三步： 
   1. neural network（function set）
   2. goodness function
   3. pick the best function

### 李宏毅：反向传播

1. 反向传播：求C对w的导数，根据链式法则就是求z对w的导数（秒算）乘上C对z的导数，所以总结规律：根据链式法则，从后往前求C对z的导数就行，so easy。

### 李宏毅：机器学习任务攻略

1. training loss太大的原因： 
   1. model bias太大（models太小）
   2. optimization issue
2. testing loss太大的原因 
   1. training loss太大
   2. overfitting（models太大）
   3. mismatch（与overfiting不同在于，不能通过增加data解决，因为traning data和testing data的分布不一样！）
3. **解决overfitting的方法：** 
   1. 增加训练资料（不要花太多力气去做这些）
   2. data augamentation，运用你的理解，去创造出新的资料。比如左右翻转，但不是上下颠倒
   3. 不要让models 太大: 
      1. 更少参数，共享参数
      2. 更少features
      3. early stopping
      4. regularization
      5. dropout（？）
4. Optimization 失败的原因： 
   1. local minima
   2. saddle point（鞍点）

### 李宏毅：local minima和saddle

1. 判断error suface到达了local minima还是saddle point的步骤： 
   1. taylor series
   2. 计算 Hessan Matrix H（$L(\theta)$对$\theta_i$和$\theta_j$的导数组成）
   3. 计算H（eigen value）的eigen value与0的关系
   4. 如果是saddle point，可以继续看H来update参数： 
      1. 找到$\lambda$=eigen value of u < 0
      2. 找到$\lambda$对应的u=eigen vector of u
      3. 用$\theta=\theta^{'} + u$得到 $\theta$的update
      4. 现状：一般很少用H来逃离，因为H很难计算！

### 李宏毅：batch和momentum

1. 思路：batch size大： 
   1. 一次update和小的batch size的时间差不多，但是一次epoch就更短（优点）
   2. gradient时更加stable，没有小的那么noisy
   3. tranining的效果更差（由于optimization更差，所以training时表现更差）（缺点）
   4. 更容易overfitting（即使控制traning时相同，testing也更差，可能更容易走到一个狭窄的峡谷里，导致容易对mismatch的结果产生偏差）（缺点）
2. 思路：momentun：每次update时不只考虑gradient，还要考虑上一次的update的方向（再加一个超参~）。这样可以加一个惯性，可能可以翻过小山丘

### 李宏毅：自动调整学习率

1. 现状：vanilla：训练的loss无法下降，往往不是到了critical point，而是别的原因，比如在谷的两边震荡！ 
   1. 思路：**ada grad**：在某个方向非常缓，那就应该设大点，另一个方向非常陡峭，learning rate应该设小点——需要一个客制化的lr 
      1. $\theta_i^{t+1} := \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t$
      2. $\sigma_i^t = \sqrt{\frac{1}{t+1}\Sigma_{j=0}^t(g_i^j)^2}$
2. 现状：同一个参数同一个方向，陡峭程度也在变化呢（如新月形状的error surface）？ 
   1. 思路：**RMS Prop** 
      1. 利用 $\alpha$ 动态调整：$\sigma_i^t = \sqrt{(1-\alpha)\sigma_i^{t-1}+\alpha g_i^t}$
   2. 拓展：**Adam grad**，也就是RMS + Momentum
3. 现状：乱喷 
   1. 思路：learning rate schedule：**lr decay**： $\eta$ 也需要随着时间的发展而减小（$\eta^t$），避免乱喷
   2. 思路：**warm up**：lr 先变大，后变小（黑科技！） 
      1. 拓展：residual network：https://arxiv.org/abs/1512.03385，一开始0.01，然后0.1
      2. 拓展：transformer：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
      3. 思路：有可能是因为为了让 $\sigma$ 统计到更多的信息
      4. 拓展：Radam：[https://arxiv.org/abs/1908.03265](https://arxiv.org/abs/1908.03265)
4. 思路：山（error surface）比较崎岖

### 李宏毅：classification（简略）

1. 现状：如何分类 
   1. 思路：将分类当作回归，将类别用数1，2，3...表示。有时可用，有时不可用。
   2. 思路：最后一步将 $c^T$换成 $W'（矩阵）$， $b$换成 $b'（向量）$，（**一个w，一个b，一个结果，变成多个w，多个b，多个结果**）通过**softmax**：$y'=softmax(b'+W'\sigma(b+Wx))$，然后和one-hot vector的结果做cross entropy（交叉熵）
   3. 思路：采用**交叉熵**，与使用maximun likelihood的效果一样？（？）
   4. 拓展：**对两个class分类时，采用softmax和采用单个sigmoid（logistic），结果是一样的！**（？）
   5. 拓展：采用cross entropy而不是mean square error，是因为当MSE远离真实值时，grad也会为0哦
   6. 总结：选好loss function很重要

### 李宏毅：batch normalization

1. 现状：如何把error surface的山铲平？ 
   1. 现状：w_1和w_2的斜率不一样，导致椭圆形的error surface 
      1. 解释：x_1和x_2的值scale差距比较大，比如x_1的值都在很小的范围，那么w_1的斜率就很小。
      2. 思路：feature normalization： $\tilde x^r := \frac{x_i^r - mean(\sum_i x)}{standard \ deviation(\sum_i x)}$ 
         1. 现状：除了第一层之外，对各层的activation function之前或之后的中间结果$z^1,z^2...$，还需要做feature normalization！否则traning还是会比较困难。对sigmoid还是比较推荐在sigmoid前做normalization。 $\tilde{z}^i = \frac{z^i-\mu^i}{\sigma^i}$
         2. 现状：将 $z^1,z^2...$等中间结果进行normalization得到$\tilde{z}^1,\tilde{z}^2...$，如果你改变了某个 $z^1$，那么不只是 $\tilde{z}^1$会改变，别的资料的结果也会跟着改变！所以要把你用来normalization的所有资料（也就是包括每次update用到的，或者说每个batch）以及 $\mu, \sigma$ ，当作一个大的神经网络
         3. 还需要通过线性处理 $\gamma, \beta$
         4. 现状：testing怎么搞？难道只有一笔资料也要减去 $\mu$ 和 $\sigma$ 吗？真正的application上没有batch这个东西 
            1. 对training中的$\mu$ 和 $\sigma$ 计算moving average。
         5. 现状：batch normalization为什么比较好？只知道它可以让error surface不崎岖，但是不知道原因，可能是serendipitous，一种偶然的发现。

### 李宏毅：classification（详细）

1. 现状：分类的损失函数，如果用 $L(f)=\Sigma_n{\delta(f(x^n)\not=\hat{y}^n)}$，无法求导 
   1. 思路：SVM等
   2. 思路：先验概率。每个类别本身也有几率Prior，这个是需要先验知识来假定的。（贝叶斯公式）——generative model，可以根据全概率公式自己产生P(x)：$P(C_1|x) = \frac{P(C_1x)}{P(x)} = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}$ 
      1. P(C1)、P(C1)：根据各个类的采样数量来决定
      2. P(x|C1)、P(x|C2)：**假设符合高斯分布**（一个特征，就是1维高斯分布，两个特征，就是2维的高斯分布），但是这个高斯分布的参数（如2维的就是向量 $\mu$ 和矩阵 $\Sigma$ ）是需要根据traning data来估计的。**估计方法就是maximun likelihood(注意，不是用交叉熵也不是用MSE)** ：$L(\mu, \Sigma)=f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)...f_{\mu,\Sigma}(x^n)$。找到 $\mu^*,\Sigma^*=\underset{\mu,\Sigma}{max}L(\mu,\Sigma)$。都有公式可以秒算，比如 $\mu^*$ 就是各个 $\mu$ 平均值
   3. 现状：用贝叶斯+高斯分布，准确率贼低！ 
      1. 思路：两个类shared一个 $\Sigma$ ：$L(\mu_1,\mu_2,\Sigma)=f_{\mu^1,\Sigma}(x^1)f_{\mu^1,\Sigma}(x^2)...f_{\mu^1,\Sigma}(x^m)f_{\mu^2,\Sigma}(x^1) \times f_{\mu^2,\Sigma}(x^2)...f_{\mu^2,\Sigma}(x^n)$。boundary就会变成一条直线——linear model。 $\mu^1和\mu^2$的算法和上面的一样。$\Sigma=\frac{1}{m}\Sigma^1+\frac{1}{n}\Sigma^2$
      2. 拓展： $P(x|C_1)$ 中的x本来是向量，如果各个维度相互独立，就可以分成多个1-D gaussion（现实中往往不独立）——naive bayes classifier
      3. 拓展：不一定用gaussion，比如一个binary features，应该用bernoulli distribution，而不是gaussion distribution
2. 现状：如果是share $\Sigma$（共用 covariance matrix）的情况，我们来看和sigmoid的关系：将贝叶斯的分子分母同时除以分子，然后通过一顿操作，可以转换为 $P(C_1|x)=\sigma(z)$, 其中$z=ln \frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}$ ，然后带入“共用 covariance matrix”这一条件，就可以化简成$z=wx+b$，function set $P(C_1|x)$就变成一个**sigmoid套一个linear function**的形式了...所以共用 $\Sigma$ 分界线就是一条直线。所以综上，能否直接将w和b直接找出来，当然可以： 
   1. 思路：（Posterior probability）logistic regression： 
      1. function set：$P_{w,b}(C_1|x)=f_{w,b}(x)=\sigma(\sum_i{w_ix_i+b})$
      2. loss functon：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^2))...f_{w,b}(x^N)$ ，并且转化成找 $w^*,b^*=arg \underset {w,b}{min}-lnL(w,b)$，于是转换成求交叉熵 $L(f)=\sum_n{C(f(x^n),\hat{y}^n)}$
      3. find the best function：对$w_i$求导，得到 $\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n$。意味着预测值与实际值差距越大。update的量也越大。而这个式子和linear regression是一样的，只不过 $\hat{y}^n$是1或1，而 $\hat{y}^n-f_{w,b}(x^n)$ 介于0和1之间。注意，这里的 $f_{w,b}(x^n)$是**通过了sigmoid的**linear function，它用了**交叉熵以后**，**再求偏微分**，刚好和**没通过sigmoid的**linear function，用了**MSE**之后再求偏微分形式一样！总结：**wx+b & Sigmoid & Cross Entropy & 偏微分 == wx+b & MSE & 偏微分**。
      4. 拓展：如果sigmoid加上MSE，那么算出来的偏导是会导致：距离目标很近的时候最然learning rate会接近0，但是距离目标很远的时候，learning rate也会接近0（这里的learning rate指的是grad，不是那个超参），会卡住。
3. 用几率模型（如gaussion）和logistic regression，如果把几率模型的covariance matrix设为shared，那么他们的function set都是一样的：$P(C_1|x)=\sigma(w \cdot x + b)$。dicriminative 和generative的function set 都是一样的，但是假设不一样，所以结果是不一样的。 
   1. 在dicriminative 里，对w和b没有任何假设
   2. 在generative里，对 分布做了假设，如gaussion、bernouli、naive bayes等（可能由于受到prior knowledge影响（？）总之generative model做了某些脑补——通常不是好事，但是data很少的时候有时候也有用！） 
      1. 思路：discriminative可能差于generative的情况： 
         1. data很少时
         2. 有比较好的假设，可以盖过噪音时
         3. 可以由各个来源获得prior知识：如语言识别，可以上网爬取大量语音，得到prior knowledge！
4. 思路：multi-class classification： 
   1. softmax分类器：各个**wx+b**的结果，取exp，然后除以各个exp值的总和——强化大的值，弱化小的值（可以像logistic regression那样（详见上面第3点的讲解）从共用covariance matrix的gaussion distribution+maximun likelihood的角度推导过来，也可以从maximun entropy的角度推导，详见bishop P209-210）
   2. 拿结果与hot-code的结果做cross entropy。
   3. 总结来说： 
      1. **logistic regression：只用一个wx+b：sigmoid(wx+b)+cross entropy相当于二维的共用covariance matrix的gaussion distribution+maximun likelihood推导而来的**（分子分母同时除以分子，取exp和取对数，展开，带入share covariance matrix这一条件，就得到sigmoid(wx+b）了)；
      2. **multi-classification：用多个w_i x+b：softmax(w_i x+b_i)+cross entropy就相当于高维的共用covariance matrix的gaussion distribution+maximun likelihood推导而来的。**
      3. 现状：logistic regression是由limitation的，例如连线交叉了 
         1. 思路1：如果坚持要用logistic regression，可以用feature transformation（不推荐！）
         2. 思路2：将多个logistic regression并联起来再输出到一个logistic regression，相当于从单个w_i x+b变成多个w_i x+b！并联的部分就相当于feature transformation！

### 李宏毅：HW2

1. 现状： 
   1. 采用TMIT corpus
   2. 输入每一帧（25ms）的MFCC特征（39 dim），来辨别phoneme
   3. 细节：一帧太短，可能捕捉不到足够的细节，所以会将其前5帧、后5帧的数据一起作为特征处理，并且flatten成1 _ 429的一个tensor。当然也可以reshape回11 _ 39
   4. 注意：回顾一下，training data和validation data的区别：update时只用了training data，一般完成一个epoch之后再用validation data来验证，确定是否要保存model
   5. 


### 李宏毅：CNN

1. 图片是3-D tensor：长、宽、channels
2. 从neuron的角度，cnn = receptive field + parameter sharing 
   1. **每个neuron只考虑自己的receptive field，而不是像fully connected那样，每个neuron都考虑完全部的input！**
   2. **每一个receptive field共用一组参数，对应neuron的参数是一样的**
   3. 总结：fully connected > receptive field > parameter sharing（CNN）
3. 从filter的角度，拿同一个filter去与图片做内积 
   1. **有多少个filter，就会有多少个neuron，也就是多少个output channels**
4. share parameter这个行为，和拿同一个filter去扫整个图片，意义是一样的
5. convolution和pooling可以交替使用 
   1. pooling的好处：减少运算量
   2. pooling的坏处：丢失信息（建议不用pooling）
6. convolution -> flatten -> fully connect
7. data augmentation，处理scaling 和 rotation （spatial transformer layer可以处理）

### 李宏毅：自注意力Self-Attention

1. 输入是可变长度的sequence（vector set）
2. 输入是vector set的包括：自然语言、语音处理、社交关系图、分子等
3. 输出可能是一个和输入一样长的向量（sequence labeling），也可能是单个值，也可能是根据模型而变化的向量
4. I saw a saw，如何分两个saw的词性？ 
   1. 把每个向量周围的一个window的咨询一起放进去训练（如音素辨识）
   2. self-attention，考虑一整个sequence，有几个输入就有几个输出 
      1. sa和fc可以交替使用
      2. 《Attention is all you need》**transformer**
5. sa：计算 $\alpha$，两个向量的关联程度： 
   1. w乘输入a，得到q, k, v
   2. q 内积 k 得到attenton score，通过softmax/ReLU等，然后右乘 v 得到输出b
6. multi-head
7. positional encoding：为不同位置设置一个向量e，然后加到输入a中——尚待研究
8. truncated self-attention，语音识别时不一定要看完一整个句子
9. **CNN = 简化版的Self-attention，CNN的弹性更小**
10. 比较有弹性的model，需要更多数据，否则容易过拟合
11. **rnn已经可以被sa取代了：sa可以“天涯若比邻”，而且可以parallel处理。**
12. 对于graph，attention matrix已藉由edge暗示，可以直接根据graph的edge（domain knowledge）直接获取attention matrix。

### 李宏毅：HW3

1. CNN
2. data augmetation
3. 充分利用unlabeled data，semi/self-supervised learning（confident enough即可）
4. torchvision.transform可以用于augmentation
5. resnet50是一些好的网络架构，附带有预训练的weight
6. 使用nn.Conv2d，做正则的时候就要使用对应的nn.BatchNorm2d
7. nn.MaxPool2d(n,n,0)，则图片的长和宽（能被n整除的话）都会变为原来的1/n
8. 由于用了batch normalization，输入是四元组（N, C, H, W），代表batch、channels、height、width，输出是二元组（N, 分类）
9. 最后记得通过softmax来获得probability distribution
10. loss.backword()和optimizer.step()之间可能需要nn.utils.clip_grad_norm_

### 李宏毅：HW4

1. 大概就是有一批资料，每条资料是一段语音，每条语音属于一个speaker，每段语音被划分为多个frame，每若干个frame会被划到一个segment里（不足的话需要pad）（每段语音只需要一个segment），**每个frame就是一个向量，代表一个feature**，可以说segment就是一个vector set。
2. 在Dataset里面的__getitem__就要划分好segment。
3. 做self attention的时候，在设计model的时候需要过一个

### 李宏毅：Transformer

1. seq2seq：speech recognization、language translation、speech transalation？chatbot
2. 很多问题可以转换成QA，QA的问题可以用seq2seq来解决（question, context -> seq2seq -> answer）
3. seq2seq 
   1. multi-label classification
   2. object detection
4. seq2seq > transformer > self attention
5. encoder：in 向量，out 同长度向量 
   1. rnn
   2. cnn
   3. self attention（transformer使用的encoder）
6. transformer 的 encoder：一个block包括多个layer，每个block里面是self attention + fc， 
   1. positional encoding
   2. block$\times$ N： 
      1. **multiself attention，add（residual）&norm（layer norm）**
      2. **fully connected（feed forward） ，add & norm**
7. decoder 
   1. autoregressive（AT）： 
      1. **begin -> masked self attention，add & norm**
      2. **和encoder的输出cross attention（可以改结构），add & norm**
      3. **fully connected，add & norm**
      4. 得到vocabulary size的vector，例如所有中文的方块字、英文的sub word，的distribution
   2. not-auto regressive（NAT）（热门方向） 
      1. 提前准备好某个长度的输入，begin
      2. advantage：parallel，controllable output length
      3. solution：another predictor来预测输入长度， a very long begin seq ...
8. teacher forcing：using the ground truth as input of decoder。 
   1. 可以造成mismatch。一步错步步错。exposure bias
   2. 解决办法：不要给完全正确的ground truth——schedule sampling for transformer
9. copy mechanism： 
   1. chat bot
   2. summarization
   3. 研究：pointer network
10. guided attention 
   1. monotonic attention
   2. location-aware attention
11. beam search：有时有用有时无用 
   1. 拓展：对于**答案明确**的任务，需要拿到最高分越好，例如语音辨识。但是对于**答案不明确**的，例如语音合成（TTS）、将故事的时候，需要加**noise**！语音合成，即使是测试，也需要加入noise！神奇。
   2. accept that nothing is perfect. True beauty lies in the cracks of imperfection.
12. 测试用cross entropy，验证用BLEU score，因为测试用BLEU score

### 李宏毅：RNN

1. slot filling（是不是类似寄件的时候复制一段文字自动填地址、姓名、电话？）
2. 每一列中间有hidden layer，和output layer，都是用sigmoid等来激活
3. memory
4. can be deep，多加几层hidden layer
5. elman vs. jordan network，将中间值/结果值传给下一个
6. bidirectional rnn，双向
7. long short-term memory：写入读取memory时都有控制，自己学input gate，output gate，还有forget gate。
8. **每个LSTM的memory cell一共4个input，都是scalar，一个output**
9. rnn：每次memory都被洗掉，很short-term；LSTM，是否保留由forget gate决定
10. input output 用乘的，forget用加的（加上一次保留的）
11. cell = neuron
12. 一般的neuron只需要**一个input**，各个标量x乘上w。LSTM的cell需要**4个input**，各个**vector x** 乘上 w
13. 实作中会由xi得到vector Z, Zi, Zf, Zo，总之一个xi就会最终去操控所有LSTM memory cell的运作，类似普通的神经网络，还有rnn
14. 实际除了把memory继承下来，还会把output和memory都加入下一次的input。
15. 然后还会叠好几层
16. keras里面有写好的LSTM、GRU、SimpleRNN

### 李宏毅：HW5

1. 采用ted的数据，训练时有中英文对应，测试时只有中文
2. 用BLEU测试
3. preprocessing： 
   1. clean（去除掌声等），normalize（把逗号等符号统一）
   2. remove bad data
   3. tokenization
4. tips： 
   1. tokenize en with **sub-word units**
   2. label smoothing regularization（计算loss的时候保留一些不是最高分的labels（引入噪声？））
5. 用sentencepiece套件将英语断词
6. 用fairseq将句子变为二进制，存储在data-bin
7. 使用fairseq的TranslationTask来setup task，得到的task可以load_dataset，使用task.get_batch_iterator来读取数据
8. 使用fairseq的model, enoder, decoder
9. encoder、（cross）attention、decoder，都是包括“参数、输入、输出”三个部分的

### 李宏毅：GAN

1. 加入simple distribution，需要足够简单，可以sample
2. 输出也就不固定，而是一个distribution
3. GAN 
   1. unconditional generator：是一个nn。输入一个简单的distribution，输出图片（一个复杂的distribution）。
   2. discriminator：是一个nn。输入一个图片，输出一个数值scalar，评分越高越好
   3. 枯叶蝶和比比鸟。generator的天敌（adversarial）就是discriminator。
4. step： 
   1. fix generator，update discriminator（classify or regression都可以）
   2. fix discriminator，update generator（G->D，fix D，update G）
5. $G^* = arg\ \underset{G}{min}\ Divergence(P_G, P_{data}) = arg\ \underset{G}{min}\ \underset{D}{max}\ V(G, D) $ 
   1. 其中，当sample的数量较多，右边的 max的值和JS divergence相关
   2. 然后D就是discriminator，G就是generator
   3. 根据V的不同，对应的divergence就不同 
      1. 原论文的是 $V = E_{y \sim G}log(D(y))+E_{y\sim data}log(1-D(y))$， 对应的就是 JS divergence
6. JS divergence不是很合适，因为PG和Pdata经常不重合（low -dim manifold），或者采样的时候sample的点不够多。而对于不overlap的点，JS divergence算出的结果都是log2
7. 换一个 divergence：Wasserstein distance：需要D~1-Lipschitz
8. spectral normalization gan（sngan）
9. gan去产生文字，很难。因为难算discriminator的微分
10. 评估： 
   1. quality：
   2. 影像辨识，查看分布是否集中：mode collapse怎么办？
   3. diversity：看一堆图片，classifier输出的平均越平均，代表多样性越好。mode dropping？
   4. inception score：同时考虑quality和diversity
   5. frechet inceotion distance（FID）
   6. 如果GAN产生的和real data一模一样，也不好
11. **conditional gan** 
   1. 如text to image
   2. discriminator除了输入generator输出的的y，还需要输入x（condition），判断y是否像真实的，还要看y和x是否匹配（需要提前给真实图片打标签）
   3. 还要把一些错误的标签打在图片上，当y接近该图片时，要告诉它这是不好的。
   4. 拓展：image to image！（image translation）、music to image！
12. unpaired（unsupervised）： 
   1. 假设要从真人变成动漫人物，但是没有label 
      1. 输入从一个gaussian sample变成从真人人脸（X domain）
      2. 但是下一步，从generator出来之后要丢进discriminator的时候，没有Pair的资料，不能仿照conditional gan，咋办？
      3. 加入**cycle gan**/disco gan/dual gan
      4. 拓展：star gan
   2. 文字风格转换：也可以cycle gan
   3. 长文章变短文章
   4. 翻译
   5. 语音辨识！

### 李宏毅：BERT（填空题）

1. self-supervised：文字做填空题、图片遮起来
2. bert原来用在encoder。
3. masking input
4. next sentence prediction（没啥用）
5. SOP：sentence order prediction
6. 教会它做填空题以后...
7. bert（Pre-train） ----fine tune（微调）----> 各式downstream tasks，潜能无限。
8. GLUE，任务集。super GLUE。自然语言处理。
9. case1，情感分析 
   1. input seq，output class：
   2. bert --> linear -->softmax，bert用pre-train的（fine-tune），linear是随机初始化（scratch）
   3. 只看CLS的输出，经过lenear（需要学习）和softmax
10. case2，词性标注： 
   1. input seq，output same as input：
   2. 看各个词对应的输出，经过linear（需要学习）和softmax
11. case3，natural language inferencee，NLI： 
   1. input 2 seqs，output class
   2. 只看CLS的输出，经过linear（需要学习）和softmax
12. case4，问答系统（作业7）： 
   1. input documents、questions，output integer s、e代表答案在文章中的位置
   2. 限制：extraction-based QA，答案在文章中
   3. CLS，question，SEP，document
   4. random initialized两个向量（需要学习），分别和document的输出做inner product，经过softmax，分别得到 s 和 e。
13. bert，pre-train就是做填空题，但是应用的时候可能要“问答题”
14. bert embryology：自己去train bert
15. bert只pre-train encoder，如何处理seq2seq？（如何做decoder） 
   1. 类似bert的思想，在encoder的输入做弄坏（mask、delete、rotation...）；然后在decoder的输出试图还原。eg：BART
   2. T5：transfer text-to-text transformer（在C4）
16. why BERT work？ 
   1. bert输出的向量能代表输入的向量的意思： 
      1. 意思相近的向量距离近
      2. 由于self attention，因此还能有上下文信息
      3. 类似CBOW，只用了linear，没用self-attention等deep learning
   2. DNA分类、蛋白质分类、音乐分类，用学会了英文填空题的BERT，来分析DNA，居然也会变好？为什么呢？？？玄学。用人类日常语言的资料来预训练的模型，居然在给DNA分类的实验中比随机的模型有更好的表现。难道DNA也在说一些人类听不懂的话吗？。其实可能是训练出来的这个BERT模型，就是一组比较通用的适合大量数据训练的模型，暗含了宇宙间比较通用的奥秘
   3. Multi-lingual BERT：multi-BERT上用英文做QA的fine-tune，但是在中文上test效果也很好！不同语言同一意思的向量难道能变得很接近呢？为什么不会被混成别的语言呢？其实咨询还是藏起来了，将输出整体加上英语和中文的average之间的差，就可以变过去。

### 李宏毅：GPT（做预测）

1. predict next token
2. 像transformer的decoder，但是只会做mask 的 self-attention
3. GPT不必要像BERT那样接一个linear 出来，有别的想法： 
   1. incontext-learning：只看一点例子甚至没有例子
   2. case1：few-shot、one-shot、zero-shot的翻译。
   3. 有些任务很好学：比如加减法
   4. 有些任务很难学：比如逻辑推理
4. GPT和BERT只是属于三种self-supervised的prediction。还有contractive和data centric
5. self-supervised还可以用在图像、语音。
6. 文字上，有GLUE这样的benchmark。但是在语音上，需要更多的benchmark！如SURERB
7. self-supervised：先不需要label，训练一下，然后再到下游downstream的任务再用label的数据进行微调fine-tune

### 李宏毅：auto-encoder

1. auto-encoder很早的self-supervised，06年就有
2. encoder+decoder
3. reconstruction，类似cycle-GAN
4. encoder将高维变低维，dimension reduction。训练的时候，往往不需要原来那么多的信息，将复杂的变有限的。
5. de-noising auto-encoder，还原加入噪音前的结果，类似BERT！**BERT其实就是de-nosing 的auto-encoder**
6. embedding/representation/code 能否从中解读各个维度代表的意思？feature disentangle：可行的
7. 应用1：feature disentangle，如voice conversion： 
   1. supervised learning：两个语者都要念
   2. auto-encoding：只需要两个人的声音，分别做auto-encoder。然后通过feature disentangle，只替换声音特征，不替换内容
8. 应用：discrete representation：VQVAE，code book，区分音标。
9. 应用：text as representation：能否不用vector作为embedding，而是用word seq。seq2seq2seq，中间还要接一个discriminator，中间的最后就是摘要。（这玩意最后就是个cycle-GAN，只不过GAN关注的是最后生成的，而这里关注中间的摘要）
10. 应用：word->tree strcuture->word
11. 应用：generator。Variational auto-encoer：把decoder当作generator来用
12. 应用：encoder压缩，decoder会解压缩
13. 应用：Anomaly detection（异常检测），detect input x is _similar_ to training data or not 
   1. 相似不相似，见仁见智
   2. 应用：fraud detection
   3. 应用：network instrusion detection
   4. 应用：cancer detection
   5. 难点：很多正常的资料，很少异常的资料（one-class）
   6. 解决：auto-encoder：training时训练一个auto-encoder，测试时计算输入与输出的差值，超过阈值就是异常。

### 李宏毅：adversarial attack

1.  图片中加入一点噪音，non-targeted、targeted，结果天差地别 
2.  non-targeted：同一个f，找x，使得f(x)离真实值越远越好。 
3.  targeted：同一个f，找x，使得f(x)离真实值越远越好、离目标值越近越好 
4.  $d(x^0,x) <= 某个值$ 
   1. L2-norm
   2. L-infinity （nice）
5.  拓展：语音的话，需要domain knowledge 
6.  找 $x^* = arg\ min_{d(x^0,x)\leq 某个值\varepsilon} L(x)$ ： 
   1.  正常gradient descent 
   2.  if $d(x^0,x) > \varepsilon$
then $x^t \leftarrow fix(x^t)$ 
7.  FGSM：fast gradient Sign Method： 
   1. 一拳超人：只update一次参数
   2. gradient里面取sign，+1/-1
   3. lr直接设置为 $\varepsilon$
8.  white box attack是知道model的 
9.  black box（不知道model）： 
   1. 如果知道训练资料data，可以训练一个proxy network
   2. 如果没有训练资料，那可以自己丢一些资料进去
   3. non-targeted 比较容易成功
   4. 即使用不同的proxy network，attack都很easy（未解之谜）
   5. 有可能是资料的问题，如果资料足够丰富可能就没那么容易attack 成功
10.  one-pixel attack 
11.  更狂的，universal adversarial attack，找出某一个pixel或者一点噪音就可以骗过所有同一个模型的辨识。（骗过监视器） 
12.  speech？natural language？ 
13.  骗机器，可以把黑的变白，也可以把白的变黑 
14.  attack in real phsical world（往往是universal attack） 
15.  adversarial reprogramming 
16.  backdoor（在训练资料开了后门，只对某一张图片会错！）（用别人的公开资料集，也危险） 
17.  Defense 
   1. passive defense： 
      1. 加个filter，把图片模糊化可能就行！
      2. image compression
      3. generator再生成一次
   2. passive defense的问题：一旦别人知道你如何模糊，就没用了
   3. 拓展：randomization，随机放大缩小，随机贴到灰色
   4. 还是有可能被知道，而且攻破
   5. proactive defense： 
      1. adversarial training：加入一些被攻击过的图片，和对应的正确的label（缺点：挡不住新的攻击，消耗大量资源）
      2. 


### 李宏毅：Explainable ML

1. 神马汉斯
2. 应用：银行贷款需要给出理由、医疗诊断、假释审查、自动驾驶的理由
3. 拓展：如果可以解释，甚至可能可以根据解释来改进，而不是爆调参数
4. 对比：linear model vs deep model：越powerful往往越难解释
5. 未必需要在路灯之下找钥匙，而是可以改变路灯的方向！
6. decision tree：interpretable and powerful？random forest！
7. explainable 的目标很不明确
8. 人脑也是黑盒子，为什么不能相信deep learning这个黑盒子呢？
9. 人往往需要理由，让人高兴的explanation就是好的...
10. local / global explanation
11. local：知道哪个区域最重要： 
   1. 方法：删除、改造每个component（一些pixel，一些token），看哪个影响最大（分数最低）
   2. 计算gradient，L(x)对于各个特征x（对图片可能是pixel）的偏微分，越大代表这个x越重要！（注意不是训练时那种对参数w的比值哦！）：Saliency Map
   3. 应用：水印、png
   4. SmoothGrad：随机加noise，再平均（让人看的爽就行）
   5. limitation：Gradient saturation：大象鼻子长度越长，长度对Loss的微分就越小 
      1. Interated gradient
12. 知道如何对输入处理得到最终答案的？ 
   1. hinton：同样的人说不同的话，在MFCC上差距很大，但是在第八层hidden network之后，差距很小
   2. probing：每层train一个classifier，但不要太好或太差
   3. probing：TTS，比hinton的潮
13. global： 
   1. 假设有一个CNN分类图片，经过filter1和2，然后输入一张图片发现说filter1变化大，就说明filter1对这个图片比较敏感。这是local explanation
   2. 假设去找到一个X，可以让filter1的变化最大，这是global explanation：gradient acent。找到detect patten。
   3. 再找到X，让某个类别被识别出来的几率最大，$X^* = arg\, max_X\, y_i$ 。结果往往是一堆噪点，不过adversarial attack里面已经展示过类似的结论了。 
      1. 解决：$X^* = arg\, max_X\, y_i + R(x)$，比如R(x)可以是白色点的数目的负数
      2. 解决：训练一个image generator：generator -> classifier。都是固定generator和classifier不动的，要看的就是classifier的explanation！然后往generator里面input一些z，知道找到分数最高的z，然后看generator出来的图片，这个就比较像人能看懂的，比较容易让人开心。
   4. 方法2：训练一个简单的模型去模拟复杂的模型，分析简单模型（还是很牵强啊...）模仿一小个区域：local interpretable：LIME

### 李宏毅：Domain Adaptation

1. domain shift： 
   1. 输入分布有变化，训练时是黑白的数字（source domain），测试是彩色（target domain）
   2. 输出的分布有变化，例如测试时分布不一样
2. domain adaptation（属于transfer learning的一个环节）
3. knowledge of target domain： 
   1. 有target domain资料，little but labeled：fine tune，注意不要overfit
   2. 有target domain资料，Large but unlabeled：**domain adversarial training**：将source和target都经过feature extractor，然后类似GAN一样尽量骗过domain classifier（discriminator），为了防止feature extractor（generator）偷懒，还需要一个label predictor。拓展：将unlabeled的数据远离分界线
   3. 有target domain资料，little but unlabeled
   4. 没有target domain资料： 
      1. domain generalization 
         1. 训练资料非常丰富
         2. 训练资料不够丰富
4. source domain 和 target domain的feature不一定非得map到一起——universal domain adaptation

### 李宏毅：Reinforcement learning

1. self-supervised和auto-encoder，都还是有label的
2. 不知道正确答案（label）的时候，考虑RL
3. 没有label，但是有reward
4. actor, action, environment, observation：action = f(observation)，目标就是award最大
5. 解析RL和deep learning的关系： 
   1. step1：actor：network（function set），输入是游戏画面，输出是每一个可以采取的行为的分数。可以是CNN, RNN, Tranformer。根据结果的distribution，选择要做的概率（有随机性）
   2. step2：define Loss。从游戏开始到结束，一个episode。reward加起来：total reward（return）——训练的目标，最大化
   3. step3：optimization。s、a的循环：trajectory。 
      1. 通常通过observation（s）和action（a），才能决定reward r。
      2. 把r集合起来，就是R，也就是total reward
      3. 很难用一般的optimizaton： 
         1. 难点1：a是随机产生的，这个network有随机性。
         2. 难点2：environment和reward根本不是network，只是个黑盒子
         3. 难点3：environment和reward也往往会有随机性
      4. 根据要执行、不要执行（系数为负数），决定loss 
         1. training data：{s1,a^}...
         2. 拓展：将二元的评估改进为分数A1e1+A2e2...
         3. 难点**就是a如何定，如何产生s和a的pair** 
            1. a如何定（即使不是每一步都有reward，而是最后一步才有reward，也可以做！前面步骤的r都是0）： 
               1. version 0：观察actor和环境互动的状况，跑many episode。每次都只看当前的s，a，reward（immediate reward）。 
                  1. 问题：reward delay？（太短视了）
               2. version 1：ai有多好，不只看r1，而要看之后所有的r，叫G（cumulated reward）。G = r1+r2+r3+... 
                  1. 问题：假设游戏特别长？（太远视了）
               3. version 2（discounted cumulated reward）：G' = r1 + ar2 + a^2r2...，0<a<1，越往后权重越低 
                  1. 问题：reward这个是相对的，如果都是给分数的，reward都是正的，就不好了。解决办法：标准化。minus baseline。
               4. **policy gradient**：需要在for循环中收集，每更新一次参数（改变actor的结构），就要重新收集一次数据（可能是从输入更新后的 s 开始，再跑一次episode？）
               5. On-Policy：被训练的actor和和环境互动的actor是同一个。 
                  1. 反例：Off-Policy：有可能让 $\theta^i$ 不需要重新收集参数。eg. proximal policy optimization（PPO）
               6. **Exploration：随机性很重要**！比如actor永远不开火，就永远不知道它好还是不好！
               7. Actor Critic 
                  1. 评估Actor的好坏（给定s，输出a）
                  2. $V^{\theta}(s)$ ，对于actor $\theta$ 看到s就能预测discounted cumulated reward（期望值）
                  3. 如何训练V： 
                     1. Monte-Carlo（MC） based approach：输入很多s很a，还有G，拿来训练
                     2. Temporal-difference（TD）：有st, at, rt, st+1就可以训练
                  4. 这个V平均的是actor（和environment？）的随机性，取得是多次游戏中的期望值
                  5. 把 $V^{\theta}(s)$ 作为训练actor时候用来正则化减去的参数：G't - V(s)
                  6. 为什么不用平均减去平均？
               8. version4：平均减去平均：rt+V(st+1) - V(st)——Advantage Actor-Critic（A2C？）
               9. Actor和critic可能会一开始共用network
               10. DQN：只用critic
               11. 

6. 和GAN很像。actor->generator，env、reward->discriminator。 
   1. 问题：discriminator是network，但env、reward不是。
7. reward shaping 
   1. Sparse reward：假设reward大多数时候都是0怎么办，比如下围棋，甚至机械臂扭螺丝
   2. 解决：define extra rewards（订一些小目标）
   3. reward shaping需要domain knowledge的
   4. curiosity based：给机器加上好奇心，要看有意义的新东西，给reward：对于马里奥，只要让他不断看到新东西，就可以！不需要reward
8. No reward？ 
   1. 游戏中才比较容易有reward
   2. Imitation learning： 
      1. Behavior cloning：给很多trajectory by expert 
         1. 缺点1：expert没有犯错的资料
         2. 缺点2：人类一些行为只是习惯，没意义（copy irrelevant behavior）
      2. inverse reinforcement learning（学习出一个reward function）： 
         1. 原来是：environment+reward function -> RL -> actor
         2. 现在是：environment + expert -> Inverse RL -> reward function（反推） 
            1. principle：teacher is better
            2. 流程： 
               1. init a actor
               2. update reward function，尝试让expert 获得更高分
               3. update actor，想办法获得高分reward
            3. actor类似generator，reward function 类似discriminator
            4. 拓展：给他一个画面，达到目标。机器会自己想象画面

### 李宏毅：Life Long Learning

1. 符合人类的想象
2. never ending learning, continuous learning, incremental learning
3. 用线上资料更新模型？labelled data->model->online->feedback->model...
4. 难点：学了粗糙的任务1，再学精细的任务2，就忘记了任务1（**catastrophic  forgetting**）
5. multi-task training？ 
   1. storage issue，computation issue。
   2. multi-task training往往就是Life Long learning的upper bound
6. 为什么不每个任务一个模型呢？ 
   1. 模型没那么多存储
   2. 各个任务不能共享资料
7. 和transfer learning 
   1. transfer learning只看第二个任务好不好，但LLL同时在意新的和旧的任务好不好
8. Evaluation： 
   1. 每学一个task，就去测试所有T个任务的正确率，形成一个TxT的表格。
   2. 标准1：计算最后的那轮之后，任务1-T的正确率的和。
   3. 标准2：Backward Transfer：Rt,1-R1,1   Rt,2-R2,2 ...（通常小于0，如果是正的，那就很厉害了）
9. 解决办法 
   1. selective synaptic plasticity： 
      1. 修改Loss，有一项用新的参数减去旧的参数，然后加一个系数b。b越大，越接近原来的结构，b越小，越容易forget
   2. gradient episodic memory 
      1. 不是修改loss，而是修改gradient的方向——参考前一个任务的资料（缺点是要存一点点资料）
   3. progressive network：拿以前的hidden layer，加入新的参数，不动旧的参数。缺点：会不停新增参数，存储难
   4. packNet：先分配大量空间，然后随着新的任务往里填
   5. generating data：训练task1的时候，除了训练一个classifier，还训练一个generator，可以产生任务1的资料！（非常有效！）
10. 问题：训练新的任务的时候，同时增加新的class！也可以的
11. curriculum learning：研究学习任务的顺序

### 李宏毅：network compression

1.  比较少量的参数，相近的效能 
2.  network pruning 
   1. weight的重要性 
      1. absolute values, like long
   2. neuron的重要性
3.  Pruning完以后还可以再fine-tune来还原，然后再pruning，反复进行 
4.  pruning weight（以参数为单位）的缺点： 
   1. 不好实作
   2. 不好GPU加速
5.  直接train小的network，比train大的network再pruning，要得到的效果差： 
   1. 大乐透假说lottery ticket hypothesis 
      1. 不改变正负号，小的network上容易训练sign-ificance
      2. weight agnostic
6.  knowledge distillation 
   1. teacher , student
   2. student就根据teacher的结果来学习
   3. teacher可以是ensemble，训练多个network，取平均
   4. temperature for softmax，加一个T（超参数），让分类的概率更为不集中，才可以把student学得好
7.  parameter quantization 
   1. using less bits：本来用16bits存一个数值，变成8个bits存一个数值
   2. weight clustering，要求训练的过程中也让weight比较接近，然后最后分集群
   3. huffman encoding：常见的用少的bits描述，少见的用多点bits描述
   4. binary weight？
8.  architecture design：depthwise seperable convolution（CNN降低参数量） 
   1. depthwise convolution：有几个channel，就放几个filter，每个filter只负责一个channel（一般的convolution的filter和channel没关系）。缺点：对跨channel的特征无能为力，解决办法如下：
   2. pointwise convolution：限制filter的大小为 1*1*channel数，只用来找到channel之间的关系
   3. 哇，居然是low rank approximation！两层之间多加一层，参数量会变少
   4. 一层拆两层
9.  dynamic computation： 
   1. 不再单纯地把network变小
   2. 能不能根据自由地调整所需要的运算资源：例如不同的device、电源量
   3. 思路1：自由地调整network的深度 
      1. 每一个layer中，多一个extra的layer。具体：让每一个layer的output和gound truth的loss越小越好！
   4. 思路2：自由调整宽度
   5. 思路3：根据图片的复杂度，让network自己决定network的深度

### 李宏毅：Meta-learning

1. 实际上自己就是在调参数
2. 业界的改进：买GPU；学界：telepathize
3. 机器能将结构、参数学出来吗？
4. 机器学习三步走： 
   1. function with unknown
   2. find a loss function
   3. opimization
5. 学习（learning algorithm）本身也是一个function，输入不是一张图片，而是一个dataset；输出就是一个classifier 
   1. 能不能将learning algorithm也学习出来呢？
6. meta learning 三步走： 
   1. step 1：what is learnable in learning algorithm（component）： 
      1. net architecture
      2. inital parameters
      3. learning rate
      4. data augmentation
      5. ......用 $\phi$ 表示learnable component，将learning algorithm写成 $F_\phi$，其中 $\phi$ 这些参数是未知的
   2. step 2：loss function $L(\phi)$ for learning algorithm 
      1. 资料是task
      2. 任务1的dataset全部扔进 $F_\phi$，然后输出的是一个classifier，然后使用“训练任务里面的测试资料”计算损失函数l
      3. 将各个任务的l加起来等于$L(\phi)$
   3. step3：通过gradient descent/rl等找到能够minimize L的 $\phi^*$ 
      1. 测试的时候，将“测试任务里面的训练资料”（little but labled）丢进去
      2. 然后测试
   4. 总之，**traning task里面包括training data和testing data，testing task里面又包括traning data和testing data**
7. outer loop，inner loop
8. few-shot learning（小样本学习），只要一点点训练资料。和meta-learning，还有一点点区别。
9. learned "learnning algrithom" 
   1. machine learning：find a function f（within-task training）
   2. meta learning：find a function F to find a function f（across-task training）
10. **哪些东西可以train**？ 
   1. init param $\theta^0$ . 
      1. MAML。相比于pretrain（pretrain包括self-supervised，还有typical的multi-task）
      2. multi-task往往就是Meta learning找init param的baseline
      3. meta-learning，和domain
   2. optimizer（learning rate...）
   3. net architecture 
      1. Network Architecture Search（NAS） 
         1. 当计算 $\phi$ 难以用gradient descent时，考虑用reinforcement learning。agent、reward
      2. DARTS，让 $\phi$ 也可以gradient descent
   4. data augmentation，也能learning
11. beyond gradient descent？直接吃traning data，直接输出network参数？invert new learning algorithm？
12. 目前还是分成两个阶段，训练和测试。可以一次吃训练和测试。learning to compare（metric-based approach）
13. meta-learnig的application？ 
   1. few-shot。N-ways K-shot（N个class，每个class又K个examples）。
   2. 需要准备很多N-way K-shot来训练
   3. Omniglot数据集可以用！

### 李宏毅：总结

1. 知识 
   1. linear model
   2. deep learning
   3. CNN
   4. self-attention（输出1个类别）
   5. transformer（输出sequence）
   6. generative model -> GAN
   7. self-supervised -> BERT
   8. train & test are different -> Domain Adaptation
   9. learn from interaction and reward -> RL
   10. Malice from humans -> Attack & Defense
   11. What does a model learn -> Explainable ML
   12. Edge computing -> Network Compression
   13. The road to Skynet -> Life-long Learning
   14. learn to learn -> Meta Learning
2. 应用： 
   1. covid-19
   2. Computer Vision 
      1. attack
      2. adaptation
      3. compression
      4. explanation
      5. anomaly detection
      6. anime face generation
   3. NLP 
      1. translation
      2. QA（BERT）
   4. Speech Processing 
      1. what is said
      2. who is saying
   5. RL
3. what to learn next 
   1. find a problem you care about and try solve it！
   2. read the papers and learn by yourself（NeurlPS, ICLR, AAAI, ICML, etc.）
4. 为学一首示子侄
